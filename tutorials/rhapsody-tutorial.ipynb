{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RHAPSODY Tutorial: From First Task to AI-HPC Workflows\n",
    "\n",
    "**RHAPSODY** — *Runtime for Heterogeneous Applications, Service Orchestration and Dynamism* — is a Python runtime system for executing heterogeneous HPC-AI workloads. It gives you a clean, async-first API to define computational tasks and run them across powerful execution backends like [Dragon](https://dragon.hpc.gov/).\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    "1. **Basic Usage** — Create your first tasks and execute them with `DragonExecutionBackendV3`.\n",
    "2. **Heterogeneous Workloads** — Mix function tasks, executable tasks, and parallel multi-process jobs in a single session.\n",
    "3. **AI-HPC Workflows** — Orchestrate a realistic AI-HPC pipeline that runs inference with vLLM and trains a model, all managed by RHAPSODY.\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before you begin, make sure you have:\n",
    "\n",
    "- Python 3.10+\n",
    "- RHAPSODY installed (`pip install rhapsody-py`)\n",
    "- Dragon runtime available (for HPC execution)\n",
    "- For Section 3: `vllm` and a compatible GPU environment\n",
    "\n",
    "### Three Core Concepts\n",
    "\n",
    "Everything in RHAPSODY revolves around three building blocks:\n",
    "\n",
    "| Concept | What It Does |\n",
    "|---|---|\n",
    "| **Task** | A unit of work — either a Python function (`ComputeTask`) or an AI inference request (`AITask`). |\n",
    "| **Backend** | The execution engine that actually runs your tasks (e.g., `DragonExecutionBackendV3`). |\n",
    "| **Session** | The orchestrator that connects tasks to backends, submits them, and tracks their state. |\n",
    "\n",
    "You define *what* to run (Task), *where* to run it (Backend), and the Session handles the *how*.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Basic Usage of RHAPSODY\n",
    "\n",
    "In this section, you will write your first RHAPSODY program. By the end, you will understand how to:\n",
    "\n",
    "- Initialize the `DragonExecutionBackendV3` backend\n",
    "- Create `ComputeTask` objects (both function-based and executable-based)\n",
    "- Submit tasks through a `Session`\n",
    "- Await results using standard `asyncio` patterns\n",
    "- Inspect task outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 — Setting Up the Environment\n",
    "\n",
    "First, import the core modules. RHAPSODY is fully async, so you will work with Python's `asyncio` throughout.\n",
    "\n",
    "- **`ComputeTask`** — represents a unit of computation (a function or an executable command).\n",
    "- **`Session`** — the orchestrator that manages task submission and lifecycle.\n",
    "- **`DragonExecutionBackendV3`** — the Dragon-based execution backend (the most performant backend, built on Dragon's Batch API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import multiprocessing as mp\n",
    "\n",
    "from rhapsody.api import ComputeTask, Session\n",
    "from rhapsody.backends import DragonExecutionBackendV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 — Your First Function Task\n",
    "\n",
    "The simplest thing you can do with RHAPSODY is run a Python function as a distributed task.\n",
    "\n",
    "Here is the workflow:\n",
    "\n",
    "1. **Define a function** — any Python callable (sync or async).\n",
    "2. **Wrap it in a `ComputeTask`** — this tells RHAPSODY *what* to run.\n",
    "3. **Create a backend** — `DragonExecutionBackendV3` will manage execution.\n",
    "4. **Open a `Session`** — the session binds tasks to the backend.\n",
    "5. **Submit and await** — RHAPSODY handles the rest.\n",
    "\n",
    "Let's define a simple function that returns the hostname of the machine it runs on. This is a classic HPC sanity check — if you're running across multiple nodes, each task may report a different hostname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_rhapsody():\n",
    "    \"\"\"A simple function that returns a greeting with the hostname.\"\"\"\n",
    "    import socket\n",
    "    return f\"Hello from {socket.gethostname()}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why is the import inside the function?**  \n",
    "> When RHAPSODY dispatches a function task to a remote worker, only the function itself is serialized and sent. Placing imports inside the function ensures they are available in the worker's execution context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 — Submitting and Running the Task\n",
    "\n",
    "Now let's put it all together. Here's the complete flow:\n",
    "\n",
    "1. Initialize the Dragon backend with `await DragonExecutionBackendV3()`.\n",
    "2. Create a `Session` and pass the backend.\n",
    "3. Wrap your function in a `ComputeTask`.\n",
    "4. Submit it and gather results.\n",
    "\n",
    "Note that `Session` supports the `async with` context manager pattern — this ensures the backend shuts down cleanly when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def basic_example():\n",
    "    # Step 1: Initialize the Dragon execution backend\n",
    "    backend = await DragonExecutionBackendV3()\n",
    "\n",
    "    # Step 2: Create a session with this backend\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    # Step 3: Define a single compute task\n",
    "    task = ComputeTask(function=hello_rhapsody)\n",
    "\n",
    "    # Step 4: Submit and await inside the session context\n",
    "    async with session:\n",
    "        futures = await session.submit_tasks([task])\n",
    "        await asyncio.gather(*futures)\n",
    "\n",
    "        # Step 5: Inspect the result\n",
    "        print(f\"Task UID  : {task.uid}\")\n",
    "        print(f\"State     : {task.state}\")\n",
    "        print(f\"Return    : {task.return_value}\")\n",
    "\n",
    "await basic_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "Let's walk through each step:\n",
    "\n",
    "| Step | What Happened |\n",
    "|------|---------------|\n",
    "| `await DragonExecutionBackendV3()` | Initialized the Dragon runtime. The backend is now ready to accept tasks. |\n",
    "| `Session(backends=[backend])` | Created an orchestration session and registered the backend. The session will route all tasks to this backend. |\n",
    "| `ComputeTask(function=hello_rhapsody)` | Wrapped your function into a task object. RHAPSODY auto-generated a unique ID (e.g., `task.000001`). |\n",
    "| `session.submit_tasks([task])` | Submitted the task to the backend and returned a list of `asyncio.Future` objects. |\n",
    "| `asyncio.gather(*futures)` | Waited for all futures (tasks) to reach a terminal state (`DONE` or `FAILED`). |\n",
    "| `task.return_value` | Retrieved the function's return value directly from the task object. Tasks are updated **in-place**. |\n",
    "\n",
    "**Expected output:**\n",
    "```\n",
    "Task UID  : task.000001\n",
    "State     : DONE\n",
    "Return    : Hello from <hostname>!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 — Scaling Up: Submitting Many Tasks at Once\n",
    "\n",
    "One of RHAPSODY's strengths is batch task submission. You can create hundreds or thousands of tasks and submit them in a single call. The backend handles scheduling, execution, and result collection for you.\n",
    "\n",
    "Let's submit 16 tasks, each returning its hostname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_worker():\n",
    "    \"\"\"Returns the hostname and process ID of the worker.\"\"\"\n",
    "    import os\n",
    "    import socket\n",
    "    return f\"{socket.gethostname()} (PID: {os.getpid()})\"\n",
    "\n",
    "\n",
    "async def batch_example():\n",
    "    backend = await DragonExecutionBackendV3()\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    # Create 16 identical tasks\n",
    "    tasks = [ComputeTask(function=identify_worker) for _ in range(16)]\n",
    "\n",
    "    async with session:\n",
    "        futures = await session.submit_tasks(tasks)\n",
    "        await asyncio.gather(*futures)\n",
    "\n",
    "        print(f\"Submitted {len(tasks)} tasks. All completed.\\n\")\n",
    "        for t in tasks:\n",
    "            print(f\"  {t.uid} -> {t.state} | {t.return_value}\")\n",
    "\n",
    "await batch_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```\n",
    "Submitted 16 tasks. All completed.\n",
    "\n",
    "  task.000001 -> DONE | node01 (PID: 12345)\n",
    "  task.000002 -> DONE | node01 (PID: 12346)\n",
    "  ...\n",
    "  task.000016 -> DONE | node02 (PID: 12360)\n",
    "```\n",
    "\n",
    "Notice that:\n",
    "- Each task received an auto-generated UID (`task.000001`, `task.000002`, ...).\n",
    "- Tasks may run on different nodes or processes — the backend distributes them automatically.\n",
    "- All tasks were submitted in one `submit_tasks()` call, which is more efficient than submitting them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 — Running Executable Tasks\n",
    "\n",
    "Not all workloads are Python functions. RHAPSODY also supports **executable tasks** — shell commands, compiled binaries, or any command-line program.\n",
    "\n",
    "To create an executable task, use the `executable` and `arguments` parameters instead of `function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def executable_example():\n",
    "    backend = await DragonExecutionBackendV3()\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    # An executable task: run a shell command\n",
    "    task = ComputeTask(\n",
    "        executable=\"/bin/hostname\",\n",
    "        arguments=[\"-f\"],  # \"-f\" flag for fully qualified domain name\n",
    "        task_backend_specific_kwargs={\"process_template\": {}},\n",
    "    )\n",
    "\n",
    "    async with session:\n",
    "        futures = await session.submit_tasks([task])\n",
    "        await asyncio.gather(*futures)\n",
    "\n",
    "        print(f\"Task UID  : {task.uid}\")\n",
    "        print(f\"State     : {task.state}\")\n",
    "        print(f\"Stdout    : {task.stdout.strip() if task.stdout else 'N/A'}\")\n",
    "        print(f\"Exit Code : {task.exit_code}\")\n",
    "\n",
    "await executable_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key differences from function tasks:**\n",
    "\n",
    "| | Function Task | Executable Task |\n",
    "|---|---|---|\n",
    "| **Defined with** | `function=my_func` | `executable=\"/path/to/binary\"` |\n",
    "| **Arguments** | `args=(...)` and `kwargs={...}` | `arguments=[\"--flag\", \"value\"]` |\n",
    "| **Result** | `task.return_value` | `task.stdout` / `task.stderr` |\n",
    "| **Backend config** | Optional | Needs `process_template` for Dragon |\n",
    "\n",
    "Notice the `task_backend_specific_kwargs={\"process_template\": {}}`. This tells `DragonExecutionBackendV3` to run the executable as a **process** (rather than a native function call). An empty dict `{}` means \"use default process settings.\" You will see more advanced configurations in Part 2.\n",
    "\n",
    "**Expected output:**\n",
    "```\n",
    "Task UID  : task.000001\n",
    "State     : DONE\n",
    "Stdout    : node01.cluster.local\n",
    "Exit Code : 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 — Session Statistics\n",
    "\n",
    "After running a workload, you can ask the session for performance statistics. This gives you aggregate information about task counts, states, and latencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stats_example():\n",
    "    backend = await DragonExecutionBackendV3()\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    tasks = [ComputeTask(function=hello_rhapsody) for _ in range(32)]\n",
    "\n",
    "    async with session:\n",
    "        futures = await session.submit_tasks(tasks)\n",
    "        await asyncio.gather(*futures)\n",
    "\n",
    "        # Get statistics from the session\n",
    "        stats = session.get_statistics()\n",
    "\n",
    "        print(\"=== Session Statistics ===\")\n",
    "        print(f\"Total tasks    : {stats['summary']['total_tasks']}\")\n",
    "        print(f\"State counts   : {dict(stats['counts'])}\")\n",
    "        print(f\"Avg total time : {stats['summary']['avg_total']:.4f}s\")\n",
    "        print(f\"Avg queue time : {stats['summary']['avg_queue']:.4f}s\")\n",
    "        print(f\"Avg exec time  : {stats['summary']['avg_execution']:.4f}s\")\n",
    "\n",
    "await stats_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```\n",
    "=== Session Statistics ===\n",
    "Total tasks    : 32\n",
    "State counts   : {'DONE': 32}\n",
    "Avg total time : 0.0523s\n",
    "Avg queue time : 0.0012s\n",
    "Avg exec time  : 0.0511s\n",
    "```\n",
    "\n",
    "This is useful for profiling your workloads and understanding where time is spent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Summary\n",
    "\n",
    "You've now covered the fundamentals:\n",
    "\n",
    "- **`ComputeTask(function=...)`** — wraps a Python callable as a distributed task.\n",
    "- **`ComputeTask(executable=...)`** — wraps a shell command or binary.\n",
    "- **`DragonExecutionBackendV3()`** — the high-performance Dragon backend.\n",
    "- **`Session`** — orchestrates submission, routing, and lifecycle tracking.\n",
    "- **`asyncio.gather(*futures)`** — waits for all tasks to complete.\n",
    "- Tasks are updated **in-place** — after awaiting, just read `task.return_value`, `task.stdout`, or `task.state`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Heterogeneous Workloads\n",
    "\n",
    "Real HPC workflows rarely consist of identical tasks. You might need to run a mix of:\n",
    "\n",
    "- Single-process Python functions\n",
    "- Multi-rank parallel functions\n",
    "- Shell commands and compiled executables\n",
    "- Parallel jobs spanning multiple process groups\n",
    "\n",
    "RHAPSODY handles this naturally. Every task is an independent object with its own configuration, and a single `Session` can dispatch them all to the same backend. The backend figures out *how* to run each task based on its configuration.\n",
    "\n",
    "In this section, you will:\n",
    "\n",
    "- Understand the **five execution modes** of `DragonExecutionBackendV3`\n",
    "- Use `task_backend_specific_kwargs` to control process layout\n",
    "- Mix different task types in one session\n",
    "- See how RHAPSODY abstracts away the complexity of heterogeneous execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 — The Five Execution Modes\n",
    "\n",
    "`DragonExecutionBackendV3` supports five distinct execution modes. The mode is determined automatically based on the task's configuration:\n",
    "\n",
    "| Mode | Trigger | What Happens |\n",
    "|------|---------|-------------|\n",
    "| **Function Native** | `function` + no backend kwargs | Function is called directly via Dragon's batch API. Fastest mode. |\n",
    "| **Function Process** | `function` + `process_template: {}` | Function runs inside a dedicated Dragon process. |\n",
    "| **Function Job** | `function` + `process_templates: [(N, {}), ...]` | Function runs as a parallel job with multiple process groups. |\n",
    "| **Executable Process** | `executable` + `process_template: {}` | Executable runs as a single Dragon process. |\n",
    "| **Executable Job** | `executable` + `process_templates: [(N, {}), ...]` | Executable runs as a parallel job with multiple process groups. |\n",
    "\n",
    "The key insight: **you don't pick a mode explicitly**. You just configure your task, and the backend infers the right execution strategy.\n",
    "\n",
    "Let's see each mode in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 — Defining the Task Functions\n",
    "\n",
    "We will create a few different functions and executables to demonstrate the five modes. Each function reports its hostname so you can see where it ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def native_function():\n",
    "    \"\"\"Runs in Function Native mode — the fastest path.\"\"\"\n",
    "    import socket\n",
    "    return f\"[native] {socket.gethostname()}\"\n",
    "\n",
    "\n",
    "async def single_process_function():\n",
    "    \"\"\"Runs inside a dedicated Dragon process.\"\"\"\n",
    "    import socket\n",
    "    return f\"[single-process] {socket.gethostname()}\"\n",
    "\n",
    "\n",
    "async def parallel_function():\n",
    "    \"\"\"Runs as a parallel job across multiple process groups.\"\"\"\n",
    "    import socket\n",
    "    return f\"[parallel] {socket.gethostname()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 — Building a Heterogeneous Workload\n",
    "\n",
    "Now, let's create one task for each execution mode. Pay close attention to the `task_backend_specific_kwargs` — this is where you control the process layout.\n",
    "\n",
    "- **No kwargs** → Function Native mode (fastest)\n",
    "- **`process_template: {}`** → Single-process mode\n",
    "- **`process_templates: [(N, {}), (M, {})]`** → Parallel job mode (N + M processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def heterogeneous_workload():\n",
    "    backend = await DragonExecutionBackendV3()\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    tasks = [\n",
    "        # --- Mode 1: Function Native ---\n",
    "        # No backend-specific kwargs → fastest execution path.\n",
    "        # The function is called directly via Dragon's batch API.\n",
    "        ComputeTask(\n",
    "            function=native_function,\n",
    "        ),\n",
    "\n",
    "        # --- Mode 2: Executable Process ---\n",
    "        # A shell command running as a single Dragon process.\n",
    "        # The empty dict {} means \"use default process settings.\"\n",
    "        ComputeTask(\n",
    "            executable=\"/bin/bash\",\n",
    "            arguments=[\"-c\", \"echo [exec-single] $(hostname)\"],\n",
    "            task_backend_specific_kwargs={\"process_template\": {}},\n",
    "        ),\n",
    "\n",
    "        # --- Mode 3: Executable Job (Parallel) ---\n",
    "        # The same command, but now running as a parallel job:\n",
    "        # 2 processes in group 1 + 2 processes in group 2 = 4 total.\n",
    "        ComputeTask(\n",
    "            executable=\"/bin/bash\",\n",
    "            arguments=[\"-c\", \"echo [exec-parallel] $(hostname) PID:$$\"],\n",
    "            task_backend_specific_kwargs={\"process_templates\": [(2, {}), (2, {})]},\n",
    "        ),\n",
    "\n",
    "        # --- Mode 4: Function Process ---\n",
    "        # A Python function running in its own dedicated process.\n",
    "        ComputeTask(\n",
    "            function=single_process_function,\n",
    "            task_backend_specific_kwargs={\"process_template\": {}},\n",
    "        ),\n",
    "\n",
    "        # --- Mode 5: Function Job (Parallel) ---\n",
    "        # A Python function running as a parallel job across\n",
    "        # 2 + 2 = 4 processes.\n",
    "        ComputeTask(\n",
    "            function=parallel_function,\n",
    "            task_backend_specific_kwargs={\"process_templates\": [(2, {}), (2, {})]},\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    async with session:\n",
    "        futures = await session.submit_tasks(tasks)\n",
    "        print(f\"Submitted {len(tasks)} heterogeneous tasks.\\n\")\n",
    "\n",
    "        await asyncio.gather(*futures)\n",
    "\n",
    "        for t in tasks:\n",
    "            # Function tasks use return_value; executable tasks use stdout\n",
    "            output = t.return_value if t.return_value else (t.stdout.strip() if t.stdout else \"N/A\")\n",
    "            print(f\"  {t.uid} | {t.state:>6} | {output}\")\n",
    "\n",
    "await heterogeneous_workload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "You submitted **five different tasks** — each with a different execution mode — in a **single `submit_tasks()` call**. The backend inspected each task's configuration and automatically chose the correct execution strategy:\n",
    "\n",
    "```\n",
    "Submitted 5 heterogeneous tasks.\n",
    "\n",
    "  task.000001 |   DONE | [native] node01\n",
    "  task.000002 |   DONE | [exec-single] node01\n",
    "  task.000003 |   DONE | [exec-parallel] node01 PID:12345\n",
    "  task.000004 |   DONE | [single-process] node01\n",
    "  task.000005 |   DONE | [parallel] node02\n",
    "```\n",
    "\n",
    "This is the power of RHAPSODY's **task abstraction**: you describe *what* you want, and the backend handles *how* to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 — Understanding `task_backend_specific_kwargs`\n",
    "\n",
    "The `task_backend_specific_kwargs` parameter is how you pass backend-specific configuration to your tasks. This is RHAPSODY's escape hatch: while the core API is backend-agnostic, this dict lets you leverage features specific to `DragonExecutionBackendV3`.\n",
    "\n",
    "Here's a quick reference:\n",
    "\n",
    "| Key | Type | Purpose |\n",
    "|-----|------|--------|\n",
    "| `process_template` | `dict` | Configuration for a single process (env, cwd, policy). Empty `{}` = defaults. |\n",
    "| `process_templates` | `list[tuple[int, dict]]` | List of `(num_ranks, config)` tuples for parallel jobs. |\n",
    "| `type` | `str` | Set to `\"mpi\"` for MPI jobs. |\n",
    "| `ranks` | `int` | Number of MPI ranks (used with `type=\"mpi\"`). |\n",
    "| `timeout` | `float` | Task timeout in seconds. |\n",
    "\n",
    "**Architecture note:** The `task_backend_specific_kwargs` pattern is intentional. RHAPSODY separates the *logical task description* (function, arguments, resource requirements) from the *physical execution details* (process templates, MPI configuration). This means the same `ComputeTask` could run on Dragon today and a different backend tomorrow — you only change the backend-specific kwargs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 — Mixing Compute Patterns in a Real Workload\n",
    "\n",
    "Let's build a more realistic example. Imagine you are running a simulation pipeline where:\n",
    "\n",
    "1. A **preprocessing** step generates input data (single-process function).\n",
    "2. A **simulation** runs across multiple processes (parallel job).\n",
    "3. A **postprocessing** step collects and summarizes results (single-process function).\n",
    "\n",
    "You can express this entire pipeline as a flat list of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(num_samples):\n",
    "    \"\"\"Generate synthetic input data for the simulation.\"\"\"\n",
    "    import random\n",
    "    data = [random.gauss(0, 1) for _ in range(num_samples)]\n",
    "    return {\"samples\": len(data), \"mean\": sum(data) / len(data)}\n",
    "\n",
    "\n",
    "async def simulate():\n",
    "    \"\"\"Run a simulation step (parallel across multiple processes).\"\"\"\n",
    "    import os\n",
    "    import socket\n",
    "    import random\n",
    "    result = sum(random.gauss(0, 1) for _ in range(1000))\n",
    "    return {\"host\": socket.gethostname(), \"pid\": os.getpid(), \"result\": round(result, 4)}\n",
    "\n",
    "\n",
    "def postprocess(task_results):\n",
    "    \"\"\"Aggregate simulation results.\"\"\"\n",
    "    return {\"num_results\": len(task_results), \"summary\": \"aggregation complete\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def simulation_pipeline():\n",
    "    backend = await DragonExecutionBackendV3()\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    # --- Phase 1: Preprocessing (single function, native mode) ---\n",
    "    preprocess_task = ComputeTask(\n",
    "        function=preprocess,\n",
    "        args=(1000,),  # Generate 1000 samples\n",
    "    )\n",
    "\n",
    "    # --- Phase 2: Simulation (4 parallel tasks, each in its own process) ---\n",
    "    simulation_tasks = [\n",
    "        ComputeTask(\n",
    "            function=simulate,\n",
    "            task_backend_specific_kwargs={\"process_template\": {}},\n",
    "        )\n",
    "        for _ in range(4)\n",
    "    ]\n",
    "\n",
    "    # --- Phase 3: Postprocessing (single function, native mode) ---\n",
    "    postprocess_task = ComputeTask(\n",
    "        function=postprocess,\n",
    "        args=([\"placeholder\"],),  # In a real pipeline, you'd pass actual results\n",
    "    )\n",
    "\n",
    "    # Submit all phases together\n",
    "    all_tasks = [preprocess_task] + simulation_tasks + [postprocess_task]\n",
    "\n",
    "    async with session:\n",
    "        futures = await session.submit_tasks(all_tasks)\n",
    "        await asyncio.gather(*futures)\n",
    "\n",
    "        print(\"=== Pipeline Results ===\")\n",
    "        print(f\"\\nPreprocessing : {preprocess_task.return_value}\")\n",
    "        print(f\"\\nSimulation results:\")\n",
    "        for t in simulation_tasks:\n",
    "            print(f\"  {t.uid} -> {t.return_value}\")\n",
    "        print(f\"\\nPostprocessing: {postprocess_task.return_value}\")\n",
    "\n",
    "await simulation_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```\n",
    "=== Pipeline Results ===\n",
    "\n",
    "Preprocessing : {'samples': 1000, 'mean': -0.0234}\n",
    "\n",
    "Simulation results:\n",
    "  task.000002 -> {'host': 'node01', 'pid': 54321, 'result': 12.3456}\n",
    "  task.000003 -> {'host': 'node01', 'pid': 54322, 'result': -8.7654}\n",
    "  task.000004 -> {'host': 'node02', 'pid': 54323, 'result': 3.2109}\n",
    "  task.000005 -> {'host': 'node02', 'pid': 54324, 'result': -1.9876}\n",
    "\n",
    "Postprocessing: {'num_results': 1, 'summary': 'aggregation complete'}\n",
    "```\n",
    "\n",
    "Notice how the simulation tasks may land on different nodes — the Dragon backend distributes them across available resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 — Functions with Arguments\n",
    "\n",
    "Tasks can pass arguments to their functions using `args` (positional) and `kwargs` (keyword). Here's a quick example showing both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power(base, exponent, label=\"result\"):\n",
    "    \"\"\"Compute base^exponent and return a labeled result.\"\"\"\n",
    "    return {label: base ** exponent}\n",
    "\n",
    "\n",
    "async def args_example():\n",
    "    backend = await DragonExecutionBackendV3()\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    tasks = [\n",
    "        ComputeTask(function=compute_power, args=(2, 10)),                        # 2^10\n",
    "        ComputeTask(function=compute_power, args=(3, 5), kwargs={\"label\": \"3^5\"}), # 3^5\n",
    "        ComputeTask(function=compute_power, args=(7, 3), kwargs={\"label\": \"7^3\"}), # 7^3\n",
    "    ]\n",
    "\n",
    "    async with session:\n",
    "        futures = await session.submit_tasks(tasks)\n",
    "        await asyncio.gather(*futures)\n",
    "\n",
    "        for t in tasks:\n",
    "            print(f\"  {t.uid} -> {t.return_value}\")\n",
    "\n",
    "await args_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```\n",
    "  task.000001 -> {'result': 1024}\n",
    "  task.000002 -> {'3^5': 243}\n",
    "  task.000003 -> {'7^3': 343}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Summary\n",
    "\n",
    "You've now seen how RHAPSODY handles heterogeneous workloads:\n",
    "\n",
    "- **Five execution modes** are available in `DragonExecutionBackendV3`, selected automatically based on task configuration.\n",
    "- **`task_backend_specific_kwargs`** is how you control the process layout — single process, multi-process job, or MPI.\n",
    "- **`process_template: {}`** runs a task in a single dedicated process.\n",
    "- **`process_templates: [(N, {}), (M, {})]`** runs a task as a parallel job with N + M processes.\n",
    "- Tasks with different configurations can be **mixed freely** in a single session and submission.\n",
    "- The **task abstraction** separates logical description from physical execution — your code stays clean while the backend handles the complexity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: AI-HPC Workflows with vLLM Inference\n",
    "\n",
    "Modern scientific computing increasingly blends traditional HPC with AI. You might want to:\n",
    "\n",
    "- Run inference with a large language model to generate synthetic data.\n",
    "- Use HPC compute tasks to preprocess, transform, or train on that data.\n",
    "- Orchestrate the entire pipeline from a single script.\n",
    "\n",
    "RHAPSODY makes this possible by supporting **multiple backends in the same session**. You can mix `DragonExecutionBackendV3` (for HPC compute) with `DragonVllmInferenceBackend` (for AI inference), and RHAPSODY will route each task to the correct backend.\n",
    "\n",
    "In this section, you will:\n",
    "\n",
    "1. Set up a vLLM inference backend alongside the Dragon execution backend.\n",
    "2. Use `AITask` to run inference prompts.\n",
    "3. Build a complete pipeline: **generate data via inference → train a model on HPC compute**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 — The Two-Backend Architecture\n",
    "\n",
    "When you create a `Session` with multiple backends, RHAPSODY uses the `backend` field on each task to decide where to send it:\n",
    "\n",
    "```\n",
    "                          ┌──────────────────────────────┐\n",
    "                          │         Session              │\n",
    "                          │                              │\n",
    "  ComputeTask ──────────► │   Route by task.backend      │\n",
    "  AITask     ──────────► │                              │\n",
    "                          │   ┌──────────┐ ┌──────────┐  │\n",
    "                          │   │ Dragon   │ │  vLLM    │  │\n",
    "                          │   │ V3       │ │ Inference│  │\n",
    "                          │   └──────────┘ └──────────┘  │\n",
    "                          └──────────────────────────────┘\n",
    "```\n",
    "\n",
    "- **`ComputeTask`** with `backend=execution_backend.name` → routed to Dragon.\n",
    "- **`AITask`** with `backend=inference_backend.name` → routed to vLLM.\n",
    "\n",
    "If a task doesn't specify a `backend`, it goes to the first backend in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 — Setting Up Both Backends\n",
    "\n",
    "Let's initialize the execution backend and the inference backend. The vLLM backend requires:\n",
    "\n",
    "- A **config file** (YAML) describing the model and hardware setup.\n",
    "- A **model name** (e.g., `\"Qwen2.5-0.5B-Instruct\"` — a small, fast model good for demos).\n",
    "- GPU configuration (`num_gpus`, `tp_size` for tensor parallelism).\n",
    "\n",
    "> **Note:** The vLLM backend needs GPU access. If you are running this on a CPU-only machine, you can still read along to understand the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import logging\n",
    "\n",
    "from rhapsody.api import AITask, ComputeTask, Session\n",
    "from rhapsody.backends import DragonExecutionBackendV3, DragonVllmInferenceBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Dragon multiprocessing start method (required once per process)\n",
    "mp.set_start_method(\"dragon\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 — Running Inference with AITask\n",
    "\n",
    "Before building the full pipeline, let's see how `AITask` works on its own.\n",
    "\n",
    "An `AITask` is similar to a `ComputeTask`, but instead of a function or executable, you provide:\n",
    "\n",
    "- A **`prompt`** — the input text (string or list of strings).\n",
    "- A **`backend`** — which inference backend to use.\n",
    "- Optional parameters: `temperature`, `max_tokens`, `top_p`, `system_prompt`, etc.\n",
    "\n",
    "After execution, the result is available via `task.return_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def inference_example():\n",
    "    # Initialize both backends\n",
    "    execution_backend = await DragonExecutionBackendV3(num_workers=4)\n",
    "\n",
    "    inference_backend = DragonVllmInferenceBackend(\n",
    "        config_file=\"config.yaml\",\n",
    "        model_name=\"Qwen2.5-0.5B-Instruct\",\n",
    "        num_nodes=1,\n",
    "        num_gpus=1,\n",
    "        tp_size=1,\n",
    "        port=8001,\n",
    "    )\n",
    "    await inference_backend.initialize()\n",
    "\n",
    "    session = Session([execution_backend, inference_backend])\n",
    "\n",
    "    # Create inference tasks\n",
    "    tasks = [\n",
    "        # Single prompt\n",
    "        AITask(\n",
    "            prompt=\"Explain what a neural network is in one sentence.\",\n",
    "            backend=inference_backend.name,\n",
    "        ),\n",
    "        # Batch of prompts (processed efficiently by the vLLM backend)\n",
    "        AITask(\n",
    "            prompt=[\n",
    "                \"Generate a random floating-point number between 0 and 1.\",\n",
    "                \"Generate a random integer between 1 and 100.\",\n",
    "            ],\n",
    "            backend=inference_backend.name,\n",
    "        ),\n",
    "        # A compute task running alongside inference\n",
    "        ComputeTask(\n",
    "            executable=\"/usr/bin/echo\",\n",
    "            arguments=[\"Hello from Dragon backend!\"],\n",
    "            backend=execution_backend.name,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Submit all tasks — RHAPSODY routes each to the correct backend\n",
    "    print(f\"Submitting {len(tasks)} mixed tasks...\")\n",
    "    await session.submit_tasks(tasks)\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for i, task in enumerate(results):\n",
    "        task_type = \"AI\" if \"prompt\" in task else \"Compute\"\n",
    "        print(f\"\\nTask {i+1} [{task_type}] ({task.get('backend')}):\")\n",
    "        print(f\"  Result: {task.return_value if task_type == 'Compute' else task.response}\")\n",
    "\n",
    "    await session.close()\n",
    "\n",
    "await inference_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output (model responses will vary):**\n",
    "```\n",
    "Submitting 3 mixed tasks...\n",
    "\n",
    "Task 1 [AI] (vllm):\n",
    "  Result: A neural network is a computational model inspired by biological neurons...\n",
    "\n",
    "Task 2 [AI] (vllm):\n",
    "  Result: ['0.7342', '42']\n",
    "\n",
    "Task 3 [Compute] (dragon):\n",
    "  Result: Hello from Dragon backend!\n",
    "```\n",
    "\n",
    "Notice how AI tasks and compute tasks were submitted together and routed to their respective backends automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 — The Full AI-HPC Pipeline: Inference → Training\n",
    "\n",
    "Now let's build something realistic. Here's the scenario:\n",
    "\n",
    "1. **Phase 1 — Data Generation (Inference):** Use a language model to generate synthetic labeled data. Each prompt asks the model to produce a data sample with features and a label.\n",
    "\n",
    "2. **Phase 2 — Model Training (HPC Compute):** Take the generated data and train a simple scikit-learn classifier on it using a Dragon compute task.\n",
    "\n",
    "This pattern — using AI to generate data, then processing it with traditional HPC — is increasingly common in scientific workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Phase 2 function: train a model on the generated data ---\n",
    "\n",
    "def train_classifier(generated_texts):\n",
    "    \"\"\"Train a simple classifier on synthetic data.\n",
    "\n",
    "    In this demo, we parse the LLM-generated text to extract numeric features,\n",
    "    then train a basic model. In a real pipeline, you would have structured\n",
    "    data from the inference step.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import json\n",
    "\n",
    "    # In practice, you would parse the LLM responses into structured data.\n",
    "    # For this demo, we generate synthetic features to demonstrate the\n",
    "    # training step running on the HPC backend.\n",
    "    n_samples = max(len(generated_texts) * 10, 100)\n",
    "\n",
    "    X = [[random.gauss(0, 1) for _ in range(4)] for _ in range(n_samples)]\n",
    "    y = [1 if sum(row) > 0 else 0 for row in X]\n",
    "\n",
    "    # Train/test split (manual, no sklearn dependency required)\n",
    "    split = int(0.8 * n_samples)\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "    # Simple nearest-centroid classifier\n",
    "    # Compute mean of each class\n",
    "    class_0 = [x for x, label in zip(X_train, y_train) if label == 0]\n",
    "    class_1 = [x for x, label in zip(X_train, y_train) if label == 1]\n",
    "\n",
    "    centroid_0 = [sum(col) / len(col) for col in zip(*class_0)] if class_0 else [0]*4\n",
    "    centroid_1 = [sum(col) / len(col) for col in zip(*class_1)] if class_1 else [0]*4\n",
    "\n",
    "    # Predict by nearest centroid\n",
    "    def predict(x):\n",
    "        d0 = sum((a - b)**2 for a, b in zip(x, centroid_0))\n",
    "        d1 = sum((a - b)**2 for a, b in zip(x, centroid_1))\n",
    "        return 0 if d0 < d1 else 1\n",
    "\n",
    "    predictions = [predict(x) for x in X_test]\n",
    "    accuracy = sum(p == t for p, t in zip(predictions, y_test)) / len(y_test)\n",
    "\n",
    "    return {\n",
    "        \"model\": \"NearestCentroid\",\n",
    "        \"n_train\": len(X_train),\n",
    "        \"n_test\": len(X_test),\n",
    "        \"accuracy\": round(accuracy, 4),\n",
    "        \"data_source\": \"LLM-generated\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ai_hpc_pipeline():\n",
    "    \"\"\"Complete AI-HPC pipeline: LLM inference → model training.\"\"\"\n",
    "\n",
    "    # --- Initialize backends ---\n",
    "    execution_backend = await DragonExecutionBackendV3(num_workers=4)\n",
    "\n",
    "    inference_backend = DragonVllmInferenceBackend(\n",
    "        config_file=\"config.yaml\",\n",
    "        model_name=\"Qwen2.5-0.5B-Instruct\",\n",
    "        num_nodes=1,\n",
    "        num_gpus=1,\n",
    "        tp_size=1,\n",
    "        port=8001,\n",
    "    )\n",
    "    await inference_backend.initialize()\n",
    "\n",
    "    session = Session([execution_backend, inference_backend])\n",
    "\n",
    "    # =========================================================\n",
    "    # PHASE 1: Generate synthetic data via LLM inference\n",
    "    # =========================================================\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PHASE 1: Generating synthetic data via LLM inference\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create prompts that ask the model to generate data samples\n",
    "    data_prompts = [\n",
    "        \"Generate a JSON object with 4 numeric features (f1, f2, f3, f4) \"\n",
    "        \"each between -2 and 2, and a binary label (0 or 1). \"\n",
    "        \"Example: {\\\"f1\\\": 0.5, \\\"f2\\\": -1.2, \\\"f3\\\": 0.8, \\\"f4\\\": -0.3, \\\"label\\\": 1}\"\n",
    "        for _ in range(10)\n",
    "    ]\n",
    "\n",
    "    inference_tasks = [\n",
    "        AITask(\n",
    "            prompt=prompt,\n",
    "            backend=inference_backend.name,\n",
    "            max_tokens=100,\n",
    "            temperature=0.8,\n",
    "        )\n",
    "        for prompt in data_prompts\n",
    "    ]\n",
    "\n",
    "    # Submit inference tasks\n",
    "    print(f\"Submitting {len(inference_tasks)} inference tasks...\")\n",
    "    await session.submit_tasks(inference_tasks)\n",
    "    await asyncio.gather(*inference_tasks)\n",
    "\n",
    "    # Collect generated data\n",
    "    generated_texts = []\n",
    "    for t in inference_tasks:\n",
    "        print(f\"  {t.uid} | {t.state} | response length: {len(str(t.response))} chars\")\n",
    "        generated_texts.append(str(t.response))\n",
    "\n",
    "    print(f\"\\nCollected {len(generated_texts)} generated samples.\")\n",
    "\n",
    "    # =========================================================\n",
    "    # PHASE 2: Train a model on the generated data (HPC compute)\n",
    "    # =========================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PHASE 2: Training a classifier on HPC compute\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    training_task = ComputeTask(\n",
    "        function=train_classifier,\n",
    "        args=(generated_texts,),\n",
    "        backend=execution_backend.name,\n",
    "    )\n",
    "\n",
    "    print(\"Submitting training task...\")\n",
    "    await session.submit_tasks([training_task])\n",
    "    await asyncio.gather(training_task)\n",
    "\n",
    "    # =========================================================\n",
    "    # Results\n",
    "    # =========================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTraining results: {training_task.return_value}\")\n",
    "\n",
    "    # Session statistics\n",
    "    stats = session.get_statistics()\n",
    "    print(f\"\\nSession stats:\")\n",
    "    print(f\"  Total tasks     : {stats['summary']['total_tasks']}\")\n",
    "    print(f\"  State counts    : {dict(stats['counts'])}\")\n",
    "    print(f\"  Avg total time  : {stats['summary']['avg_total']:.4f}s\")\n",
    "\n",
    "    await session.close()\n",
    "\n",
    "await ai_hpc_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "You built and executed a **two-phase AI-HPC pipeline**:\n",
    "\n",
    "```\n",
    "Phase 1 (Inference)              Phase 2 (Compute)\n",
    "┌───────────────────┐            ┌───────────────────┐\n",
    "│  10 x AITask      │            │  1 x ComputeTask  │\n",
    "│  (vLLM backend)   │──results──►│  (Dragon backend)  │\n",
    "│  Generate data    │            │  Train classifier  │\n",
    "└───────────────────┘            └───────────────────┘\n",
    "```\n",
    "\n",
    "- **Phase 1** submitted 10 `AITask` prompts to the vLLM inference backend. The backend batched them efficiently and returned generated text.\n",
    "- **Phase 2** took the generated text and submitted a `ComputeTask` to the Dragon backend, which trained a simple classifier.\n",
    "- The **Session** handled routing automatically — AI tasks went to vLLM, compute tasks went to Dragon.\n",
    "\n",
    "**Expected output:**\n",
    "```\n",
    "============================================================\n",
    "PHASE 1: Generating synthetic data via LLM inference\n",
    "============================================================\n",
    "Submitting 10 inference tasks...\n",
    "  task.000001 | DONE | response length: 87 chars\n",
    "  task.000002 | DONE | response length: 92 chars\n",
    "  ...\n",
    "Collected 10 generated samples.\n",
    "\n",
    "============================================================\n",
    "PHASE 2: Training a classifier on HPC compute\n",
    "============================================================\n",
    "Submitting training task...\n",
    "\n",
    "============================================================\n",
    "PIPELINE COMPLETE\n",
    "============================================================\n",
    "\n",
    "Training results: {'model': 'NearestCentroid', 'n_train': 80, 'n_test': 20, 'accuracy': 0.75, 'data_source': 'LLM-generated'}\n",
    "\n",
    "Session stats:\n",
    "  Total tasks     : 11\n",
    "  State counts    : {'DONE': 11}\n",
    "  Avg total time  : 0.3421s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 — Understanding the vLLM Backend Configuration\n",
    "\n",
    "The `DragonVllmInferenceBackend` has several important configuration options:\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `config_file` | `str` | Path to YAML config file for the model and hardware |\n",
    "| `model_name` | `str` | HuggingFace model identifier (e.g., `\"Qwen2.5-0.5B-Instruct\"`) |\n",
    "| `num_nodes` | `int` | Number of nodes for the inference service |\n",
    "| `num_gpus` | `int` | Number of GPUs per node |\n",
    "| `tp_size` | `int` | Tensor parallelism size (split model across GPUs) |\n",
    "| `port` | `int` | Port for the HTTP inference service |\n",
    "| `max_batch_size` | `int` | Maximum number of requests to batch together (default: 1024) |\n",
    "| `max_batch_wait_ms` | `int` | Max wait time in ms before dispatching a batch (default: 500) |\n",
    "\n",
    "### Batching Strategy\n",
    "\n",
    "The vLLM backend automatically batches inference requests for efficiency:\n",
    "\n",
    "1. Requests accumulate for up to `max_batch_wait_ms` milliseconds.\n",
    "2. OR the batch is dispatched immediately when it reaches `max_batch_size`.\n",
    "3. Responses are distributed back to their individual tasks.\n",
    "\n",
    "This means you get efficient GPU utilization even when submitting many small prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 — AITask Parameters Reference\n",
    "\n",
    "Here's a quick reference for all `AITask` parameters:\n",
    "\n",
    "```python\n",
    "AITask(\n",
    "    prompt=\"Your input text\",            # Required: string or list of strings\n",
    "    backend=inference_backend.name,      # Required: target backend name\n",
    "    model=\"Qwen2.5-0.5B-Instruct\",       # Model name (if not using backend default)\n",
    "    system_prompt=\"You are a helpful..\", # System-level instructions\n",
    "    temperature=0.7,                     # Sampling temperature (0.0 - 2.0)\n",
    "    max_tokens=256,                      # Maximum tokens to generate\n",
    "    top_p=0.9,                           # Nucleus sampling (0.0 - 1.0)\n",
    "    top_k=50,                            # Top-k sampling\n",
    "    stop_sequences=[\"\\n\\n\"],             # Stop generation at these sequences\n",
    ")\n",
    "```\n",
    "\n",
    "After execution:\n",
    "- `task.return_value` — the generated response.\n",
    "- `task.state` — `\"DONE\"` or `\"FAILED\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Summary\n",
    "\n",
    "You've now built a complete AI-HPC pipeline:\n",
    "\n",
    "- **`DragonVllmInferenceBackend`** provides GPU-accelerated inference with automatic request batching.\n",
    "- **`AITask`** wraps inference requests with familiar parameters (`prompt`, `temperature`, `max_tokens`).\n",
    "- **Multi-backend sessions** let you mix AI inference and HPC compute in a single workflow.\n",
    "- RHAPSODY **routes tasks automatically** based on the `backend` field.\n",
    "- You can chain phases: use inference outputs as inputs to compute tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this tutorial, you progressed from your first RHAPSODY task to a full AI-HPC pipeline:\n",
    "\n",
    "| Section | What You Learned |\n",
    "|---------|------------------|\n",
    "| **Part 1** | Core concepts — `ComputeTask`, `Session`, `DragonExecutionBackendV3`. How to submit function and executable tasks. |\n",
    "| **Part 2** | Heterogeneous workloads — mixing execution modes, using `task_backend_specific_kwargs` to control process layout. |\n",
    "| **Part 3** | AI-HPC workflows — combining vLLM inference with HPC compute, multi-backend sessions, building realistic pipelines. |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Three building blocks**: Task → Backend → Session. That's all you need.\n",
    "2. **Tasks are self-describing**: Configure them once; the backend figures out how to run them.\n",
    "3. **Async-first**: RHAPSODY uses `asyncio` throughout — `await`, `gather`, and context managers.\n",
    "4. **In-place updates**: Tasks are updated directly after execution. No need to extract results from a separate object.\n",
    "5. **Backend-agnostic design**: The core API stays the same regardless of which backend you use. Backend-specific details go in `task_backend_specific_kwargs`.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore the [RHAPSODY documentation](https://github.com/radical-cybertools/rhapsody) for advanced configuration.\n",
    "- Try scaling up to thousands of tasks with `DragonExecutionBackendV3`.\n",
    "- Integrate with [RADICAL AsyncFlow](https://github.com/radical-cybertools) for complex task dependency graphs.\n",
    "- Experiment with different vLLM models and batch sizes for your inference workloads.\n",
    "\n",
    "Happy computing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
