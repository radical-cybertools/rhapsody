{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Resource Placement with Dragon and RHAPSODY\n",
    "\n",
    "This tutorial teaches you how to control **where** your tasks run — which node, which CPUs, which GPUs — using Dragon's [Policy](https://dragonhpc.github.io/dragon/doc/_build/html/ref/dragon.infrastructure.policy.Policy.html#dragon.infrastructure.policy.Policy) system through RHAPSODY's `ComputeTask` API.\n",
    "\n",
    "You will learn:\n",
    "\n",
    "1. How to run a basic task with no placement control\n",
    "2. How to use `ProcessTemplate` for single-process tasks\n",
    "3. How to use `process_templates` for parallel multi-process jobs\n",
    "4. How to pin tasks to specific CPUs with `cpu_affinity`\n",
    "5. How to pin tasks to specific GPUs with `gpu_affinity`\n",
    "6. How to combine CPU and GPU affinity for full resource control\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- RHAPSODY installed (`pip install rhapsody-py[dragon]`)\n",
    "- Dragon runtime available\n",
    "- Launch this notebook with: `dragon jupyter lab`\n",
    "\n",
    "### Key Concept: Dragon Policy\n",
    "\n",
    "A [`Policy`](https://dragonhpc.github.io/dragon/doc/_build/html/ref/dragon.infrastructure.policy.Policy.html#dragon.infrastructure.policy.Policy) tells Dragon **where** and **how** to place a process:\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `placement` | `Policy.Placement` | Which node to target (`DEFAULT`, `HOST_NAME`, `HOST_ID`, `ANYWHERE`) |\n",
    "| `host_name` | `str` | Specific hostname (used with `Placement.HOST_NAME`) |\n",
    "| `host_id` | `int` | Specific host ID (used with `Placement.HOST_ID`) |\n",
    "| `cpu_affinity` | `list[int]` | List of CPU core IDs to bind the process to |\n",
    "| `gpu_affinity` | `list[int]` | List of GPU device IDs to bind the process to |\n",
    "| `distribution` | `Policy.Distribution` | How to distribute across nodes (`ROUNDROBIN`, `BLOCK`) |\n",
    "\n",
    "You pass a `Policy` into a `ProcessTemplate`, and then pass the template into your `ComputeTask` via `task_backend_specific_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import multiprocessing as mp\n",
    "\n",
    "from dragon.infrastructure.policy import Policy\n",
    "from dragon.native.machine import Node, System\n",
    "\n",
    "from rhapsody.api import ComputeTask, Session\n",
    "from rhapsody.backends import DragonExecutionBackendV3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "---\n\n## 1. Baseline: A Task with No Placement Control\n\nWhen you submit a task without any `Policy`, Dragon places it **anywhere** in the allocation using its default scheduling. This is the simplest case — you get no control over which node, CPU, or GPU runs your task.\n\n**Important:** There are two execution modes for function tasks:\n\n| Mode | Trigger | `return_value` | `stdout` |\n|------|---------|----------------|----------|\n| **Function Native** | No `process_template` | Python return value | `None` |\n| **Function Process** | With `process_template` | Process exit code (int) | Captured from `print()` |\n\nWhen using `process_template` for placement control, your function runs inside a Dragon process. To get data back, **print your results** and read them from `task.stdout`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n\ndef report_placement():\n    \"\"\"Report where this task is running (prints JSON for process mode).\"\"\"\n    import os\n    import socket\n    result = {\n        \"hostname\": socket.gethostname(),\n        \"pid\": os.getpid(),\n        \"cuda_visible\": os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"not set\"),\n    }\n    print(json.dumps(result))\n    return result\n\n\nasync def baseline_example():\n    backend = await DragonExecutionBackendV3()\n    session = Session(backends=[backend])\n\n    # No backend-specific kwargs → Function Native mode\n    # return_value contains the Python dict directly\n    task = ComputeTask(function=report_placement)\n\n    async with session:\n        await session.submit_tasks([task])\n        await asyncio.gather(task)\n\n        print(f\"Task {task.uid} (native mode):\")\n        print(f\"  return_value type: {type(task.return_value).__name__}\")\n        print(f\"  result: {task.return_value}\")\n\nawait baseline_example()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "The task ran successfully, but you had no say in where it landed. Let's change that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Using `process_template` for Single-Process Tasks\n",
    "\n",
    "To control placement for a single-process task, pass a `process_template` dict inside `task_backend_specific_kwargs`. This dict is forwarded directly to Dragon's [`ProcessTemplate`](https://dragonhpc.github.io/dragon/doc/_build/html/ref/dragon.native.process.ProcessTemplate.html) constructor.\n",
    "\n",
    "The most important key is `policy` — a `Policy` object that specifies the placement.\n",
    "\n",
    "### How it connects:\n",
    "\n",
    "```\n",
    "ComputeTask(\n",
    "    function=my_func,\n",
    "    task_backend_specific_kwargs={\n",
    "        \"process_template\": {         # → becomes ProcessTemplate(**this_dict)\n",
    "            \"policy\": Policy(...)      # → placement rules\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "RHAPSODY's `DragonExecutionBackendV3.build_task()` merges your template config with the task's `args`/`kwargs` and creates the `ProcessTemplate` for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "async def single_process_placement():\n    backend = await DragonExecutionBackendV3()\n    session = Session(backends=[backend])\n\n    # Discover the first node in the allocation\n    sys = System()\n    first_node = Node(list(sys.nodes)[0])\n    hostname = first_node.hostname\n    print(f\"Targeting node: {hostname}\")\n\n    # Create a policy that pins the task to that specific node\n    policy = Policy(\n        placement=Policy.Placement.HOST_NAME,\n        host_name=hostname,\n    )\n\n    # Pass the policy via process_template\n    # Note: with process_template, return_value is the exit code (int)\n    # The function's print() output is captured in task.stdout\n    task = ComputeTask(\n        function=report_placement,\n        task_backend_specific_kwargs={\n            \"process_template\": {\"policy\": policy}\n        },\n    )\n\n    async with session:\n        await session.submit_tasks([task])\n        await asyncio.gather(task)\n\n        # Parse the JSON from stdout (not return_value!)\n        result = json.loads(task.stdout.strip())\n        # Note: Dragon may report hostname differently than the FQDN\n        # (e.g., \"localhost\" vs \"node01.cluster.local\")\n        print(f\"Task ran on: {result['hostname']} (target was: {hostname})\")\n\nawait single_process_placement()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Using `process_templates` for Parallel Jobs\n",
    "\n",
    "For multi-process parallel jobs, use `process_templates` (plural). This is a list of `(nranks, template_config)` tuples, where each tuple defines a group of processes:\n",
    "\n",
    "```python\n",
    "task_backend_specific_kwargs={\n",
    "    \"process_templates\": [\n",
    "        (2, {\"policy\": policy_a}),  # 2 processes with policy_a\n",
    "        (2, {\"policy\": policy_b}),  # 2 processes with policy_b\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "This creates a Dragon **Job** with 4 total processes split into two groups. Each group can have its own placement policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "async def parallel_job_placement():\n    backend = await DragonExecutionBackendV3()\n    session = Session(backends=[backend])\n\n    # Get available nodes\n    sys = System()\n    nodes = [Node(huid) for huid in sys.nodes]\n    print(f\"Available nodes: {[n.hostname for n in nodes]}\")\n\n    # Create one policy per node (or reuse the first if single-node)\n    policy_a = Policy(\n        placement=Policy.Placement.HOST_NAME,\n        host_name=nodes[0].hostname,\n    )\n    policy_b = Policy(\n        placement=Policy.Placement.HOST_NAME,\n        host_name=nodes[-1].hostname,  # Last node (same as first if single-node)\n    )\n\n    # Parallel job: 2 processes on node A + 2 processes on node B\n    # Each process will print its placement info to stdout\n    task = ComputeTask(\n        function=report_placement,\n        task_backend_specific_kwargs={\n            \"process_templates\": [\n                (2, {\"policy\": policy_a}),\n                (2, {\"policy\": policy_b}),\n            ]\n        },\n    )\n\n    async with session:\n        await session.submit_tasks([task])\n        await asyncio.gather(task)\n\n        print(f\"Task {task.uid}: {task.state}\")\n        print(f\"Exit code: {task.return_value}\")\n        # stdout contains output from all processes in the job\n        if task.stdout:\n            print(f\"Stdout:\\n{task.stdout}\")\n\nawait parallel_job_placement()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. CPU Affinity: Pinning Tasks to Specific Cores\n",
    "\n",
    "Use `cpu_affinity` in your `Policy` to bind a process to specific CPU cores. This is critical for:\n",
    "\n",
    "- **NUMA-aware placement** — keeping memory and compute on the same socket\n",
    "- **Avoiding contention** — ensuring tasks don't compete for the same cores\n",
    "- **Reproducible performance** — eliminating OS scheduler variability\n",
    "\n",
    "`cpu_affinity` takes a list of integer core IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "def report_cpu_affinity():\n    \"\"\"Report which CPU cores this process can use (prints JSON).\"\"\"\n    import json\n    import os\n    import socket\n    result = {\n        \"hostname\": socket.gethostname(),\n        \"pid\": os.getpid(),\n        \"cpu_affinity\": list(os.sched_getaffinity(0)),\n    }\n    print(json.dumps(result))\n    return result\n\n\nasync def cpu_affinity_example():\n    backend = await DragonExecutionBackendV3()\n    session = Session(backends=[backend])\n\n    # Pin task A to cores 0-3, task B to cores 4-7\n    policy_a = Policy(cpu_affinity=[0, 1, 2, 3])\n    policy_b = Policy(cpu_affinity=[4, 5, 6, 7])\n\n    task_a = ComputeTask(\n        function=report_cpu_affinity,\n        task_backend_specific_kwargs={\n            \"process_template\": {\"policy\": policy_a}\n        },\n    )\n    task_b = ComputeTask(\n        function=report_cpu_affinity,\n        task_backend_specific_kwargs={\n            \"process_template\": {\"policy\": policy_b}\n        },\n    )\n\n    async with session:\n        await session.submit_tasks([task_a, task_b])\n        await asyncio.gather(task_a, task_b)\n\n        # Parse results from stdout\n        result_a = json.loads(task_a.stdout.strip())\n        result_b = json.loads(task_b.stdout.strip())\n\n        print(f\"Task A cores: {result_a['cpu_affinity']}\")\n        print(f\"Task B cores: {result_b['cpu_affinity']}\")\n\n        cores_a = set(result_a[\"cpu_affinity\"])\n        cores_b = set(result_b[\"cpu_affinity\"])\n        if cores_a.isdisjoint(cores_b):\n            print(\"No overlap — CPU pinning verified.\")\n        else:\n            print(\"Note: CPU affinity may not be enforced by Dragon Batch API on all systems.\")\n\nawait cpu_affinity_example()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### CPU Affinity in Parallel Jobs\n",
    "\n",
    "You can assign different CPU affinities to each process group in a parallel job. This is useful when different ranks have different compute patterns (e.g., one group does I/O-heavy work on fewer cores, another does compute-heavy work on many cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "async def cpu_affinity_parallel():\n    backend = await DragonExecutionBackendV3()\n    session = Session(backends=[backend])\n\n    # Group 1: 2 processes pinned to cores 0-3\n    # Group 2: 2 processes pinned to cores 4-7\n    policy_group1 = Policy(cpu_affinity=[0, 1, 2, 3])\n    policy_group2 = Policy(cpu_affinity=[4, 5, 6, 7])\n\n    task = ComputeTask(\n        function=report_cpu_affinity,\n        task_backend_specific_kwargs={\n            \"process_templates\": [\n                (2, {\"policy\": policy_group1}),\n                (2, {\"policy\": policy_group2}),\n            ]\n        },\n    )\n\n    async with session:\n        await session.submit_tasks([task])\n        await asyncio.gather(task)\n\n        print(f\"Task {task.uid}: {task.state}\")\n        print(f\"Exit code: {task.return_value}\")\n        if task.stdout:\n            print(f\"Stdout:\\n{task.stdout}\")\n\nawait cpu_affinity_parallel()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. GPU Affinity: Pinning Tasks to Specific GPUs\n",
    "\n",
    "Use `gpu_affinity` in your `Policy` to bind a process to specific GPU devices. Dragon sets `CUDA_VISIBLE_DEVICES` automatically based on your policy.\n",
    "\n",
    "For more details, see the Dragon documentation on [Controlling GPU Affinity](https://dragonhpc.github.io/dragon/doc/_build/html/uses/gpus.html).\n",
    "\n",
    "### Discovering Available GPUs\n",
    "\n",
    "First, let's discover what GPUs are available across the allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_gpus():\n",
    "    \"\"\"Discover all GPUs across all nodes in the Dragon allocation.\"\"\"\n",
    "    all_gpus = []\n",
    "    sys = System()\n",
    "    for huid in sys.nodes:\n",
    "        node = Node(huid)\n",
    "        for gpu_id in node.gpus:\n",
    "            all_gpus.append((node.hostname, gpu_id))\n",
    "    return all_gpus\n",
    "\n",
    "\n",
    "gpus = discover_gpus()\n",
    "print(f\"Found {len(gpus)} GPUs:\")\n",
    "for hostname, gpu_id in gpus:\n",
    "    print(f\"  {hostname} — GPU {gpu_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Pinning a Single Task to a Specific GPU\n",
    "\n",
    "Create a `Policy` with `gpu_affinity=[gpu_id]` and `placement=HOST_NAME` to pin the process to a specific GPU on a specific node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "def report_gpu():\n    \"\"\"Report which GPU this process can see (prints JSON).\"\"\"\n    import json\n    import os\n    import socket\n    result = {\n        \"hostname\": socket.gethostname(),\n        \"pid\": os.getpid(),\n        \"CUDA_VISIBLE_DEVICES\": os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"not set\"),\n    }\n    print(json.dumps(result))\n    return result\n\n\nasync def single_gpu_example():\n    backend = await DragonExecutionBackendV3()\n    session = Session(backends=[backend])\n\n    gpus = discover_gpus()\n    if not gpus:\n        print(\"No GPUs found — skipping.\")\n        return\n\n    # Pin to the first GPU\n    hostname, gpu_id = gpus[0]\n    print(f\"Pinning task to {hostname} GPU {gpu_id}\")\n\n    policy = Policy(\n        placement=Policy.Placement.HOST_NAME,\n        host_name=hostname,\n        gpu_affinity=[gpu_id],\n    )\n\n    task = ComputeTask(\n        function=report_gpu,\n        task_backend_specific_kwargs={\n            \"process_template\": {\"policy\": policy}\n        },\n    )\n\n    async with session:\n        await session.submit_tasks([task])\n        await asyncio.gather(task)\n\n        result = json.loads(task.stdout.strip())\n        print(f\"Result: {result}\")\n\nawait single_gpu_example()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### One Task Per GPU (Round-Robin)\n",
    "\n",
    "A common pattern is to submit one task per GPU, distributing work evenly across all available accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "async def one_task_per_gpu():\n    backend = await DragonExecutionBackendV3()\n    session = Session(backends=[backend])\n\n    gpus = discover_gpus()\n    if not gpus:\n        print(\"No GPUs found — skipping.\")\n        return\n\n    # Create one task per GPU, each pinned to its GPU\n    tasks = []\n    for hostname, gpu_id in gpus:\n        policy = Policy(\n            placement=Policy.Placement.HOST_NAME,\n            host_name=hostname,\n            gpu_affinity=[gpu_id],\n        )\n        task = ComputeTask(\n            function=report_gpu,\n            task_backend_specific_kwargs={\n                \"process_template\": {\"policy\": policy}\n            },\n        )\n        tasks.append(task)\n\n    async with session:\n        await session.submit_tasks(tasks)\n        await asyncio.gather(*tasks)\n\n        print(f\"Submitted {len(tasks)} tasks (one per GPU):\")\n        for t in tasks:\n            result = json.loads(t.stdout.strip())\n            print(f\"  {t.uid} — {result['hostname']} GPU {result['CUDA_VISIBLE_DEVICES']}\")\n\nawait one_task_per_gpu()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Many Tasks with Round-Robin GPU Assignment\n",
    "\n",
    "When you have more tasks than GPUs, assign them round-robin. This is a common HPC pattern for maximizing GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "def make_gpu_policies(all_gpus, nprocs):\n    \"\"\"Create per-process policies with round-robin GPU assignment.\"\"\"\n    policies = []\n    for i in range(nprocs):\n        hostname, gpu_id = all_gpus[i % len(all_gpus)]\n        policies.append(\n            Policy(\n                placement=Policy.Placement.HOST_NAME,\n                host_name=hostname,\n                gpu_affinity=[gpu_id],\n            )\n        )\n    return policies\n\n\nasync def round_robin_gpu():\n    backend = await DragonExecutionBackendV3()\n    session = Session(backends=[backend])\n\n    gpus = discover_gpus()\n    if not gpus:\n        print(\"No GPUs found — skipping.\")\n        return\n\n    num_tasks = 8\n    policies = make_gpu_policies(gpus, num_tasks)\n\n    tasks = [\n        ComputeTask(\n            function=report_gpu,\n            task_backend_specific_kwargs={\n                \"process_template\": {\"policy\": policies[i]}\n            },\n        )\n        for i in range(num_tasks)\n    ]\n\n    async with session:\n        await session.submit_tasks(tasks)\n        await asyncio.gather(*tasks)\n\n        print(f\"{num_tasks} tasks across {len(gpus)} GPUs (round-robin):\")\n        for t in tasks:\n            result = json.loads(t.stdout.strip())\n            print(f\"  {t.uid} — {result['hostname']} GPU {result['CUDA_VISIBLE_DEVICES']}\")\n\nawait round_robin_gpu()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### GPU Affinity in Parallel Jobs\n",
    "\n",
    "For multi-process jobs where each process group needs its own GPU, use `process_templates` with per-group GPU policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "async def parallel_gpu_job():\n    backend = await DragonExecutionBackendV3()\n    session = Session(backends=[backend])\n\n    gpus = discover_gpus()\n    if len(gpus) < 2:\n        print(f\"Need at least 2 GPUs, found {len(gpus)} — skipping.\")\n        return\n\n    # Group 1: 2 processes on GPU 0\n    # Group 2: 2 processes on GPU 1\n    policy_gpu0 = Policy(\n        placement=Policy.Placement.HOST_NAME,\n        host_name=gpus[0][0],\n        gpu_affinity=[gpus[0][1]],\n    )\n    policy_gpu1 = Policy(\n        placement=Policy.Placement.HOST_NAME,\n        host_name=gpus[1][0],\n        gpu_affinity=[gpus[1][1]],\n    )\n\n    task = ComputeTask(\n        function=report_gpu,\n        task_backend_specific_kwargs={\n            \"process_templates\": [\n                (2, {\"policy\": policy_gpu0}),\n                (2, {\"policy\": policy_gpu1}),\n            ]\n        },\n    )\n\n    async with session:\n        await session.submit_tasks([task])\n        await asyncio.gather(task)\n\n        print(f\"Parallel job with 2 GPU groups:\")\n        print(f\"  Task {task.uid}: {task.state}\")\n        print(f\"  Exit code: {task.return_value}\")\n        if task.stdout:\n            print(f\"  Stdout:\\n{task.stdout}\")\n\nawait parallel_gpu_job()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Combined CPU + GPU Affinity\n",
    "\n",
    "For maximum control, combine `cpu_affinity` and `gpu_affinity` in the same `Policy`. This is important for:\n",
    "\n",
    "- **NUMA-aware GPU placement** — pin the process to CPUs on the same NUMA domain as the GPU\n",
    "- **Data pipeline tasks** — dedicate specific cores for data loading alongside GPU compute\n",
    "- **Multi-tenant nodes** — partition resources cleanly between users or jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "def report_full_placement():\n    \"\"\"Report CPU affinity, GPU visibility, and hostname (prints JSON).\"\"\"\n    import json\n    import os\n    import socket\n    result = {\n        \"hostname\": socket.gethostname(),\n        \"pid\": os.getpid(),\n        \"cpu_affinity\": list(os.sched_getaffinity(0)),\n        \"CUDA_VISIBLE_DEVICES\": os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"not set\"),\n    }\n    print(json.dumps(result))\n    return result\n\n\nasync def combined_affinity_example():\n    backend = await DragonExecutionBackendV3()\n    session = Session(backends=[backend])\n\n    gpus = discover_gpus()\n    if not gpus:\n        print(\"No GPUs found — skipping.\")\n        return\n\n    # Task 1: GPU 0 with cores 0-7 (e.g., NUMA domain 0)\n    policy_a = Policy(\n        placement=Policy.Placement.HOST_NAME,\n        host_name=gpus[0][0],\n        cpu_affinity=list(range(0, 8)),\n        gpu_affinity=[gpus[0][1]],\n    )\n\n    task_a = ComputeTask(\n        function=report_full_placement,\n        task_backend_specific_kwargs={\n            \"process_template\": {\"policy\": policy_a}\n        },\n    )\n\n    tasks = [task_a]\n\n    # Task 2: GPU 1 with cores 8-15 (e.g., NUMA domain 1) — if available\n    if len(gpus) >= 2:\n        policy_b = Policy(\n            placement=Policy.Placement.HOST_NAME,\n            host_name=gpus[1][0],\n            cpu_affinity=list(range(8, 16)),\n            gpu_affinity=[gpus[1][1]],\n        )\n        task_b = ComputeTask(\n            function=report_full_placement,\n            task_backend_specific_kwargs={\n                \"process_template\": {\"policy\": policy_b}\n            },\n        )\n        tasks.append(task_b)\n\n    async with session:\n        await session.submit_tasks(tasks)\n        await asyncio.gather(*tasks)\n\n        for t in tasks:\n            result = json.loads(t.stdout.strip())\n            print(f\"Task {t.uid}:\")\n            print(f\"  Host : {result['hostname']}\")\n            print(f\"  CPUs : {result['cpu_affinity']}\")\n            print(f\"  GPU  : {result['CUDA_VISIBLE_DEVICES']}\")\n            print()\n\nawait combined_affinity_example()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "---\n\n## Quick Reference\n\n### Policy Parameters\n\n```python\nfrom dragon.infrastructure.policy import Policy\n\npolicy = Policy(\n    placement=Policy.Placement.HOST_NAME,  # DEFAULT, ANYWHERE, HOST_NAME, HOST_ID\n    host_name=\"node01\",                    # Target node (with HOST_NAME)\n    cpu_affinity=[0, 1, 2, 3],             # Pin to specific CPU cores\n    gpu_affinity=[0],                      # Pin to specific GPU devices\n    distribution=Policy.Distribution.ROUNDROBIN,  # ROUNDROBIN, BLOCK\n)\n```\n\n### Passing Policies to RHAPSODY Tasks\n\n```python\n# Single process\nComputeTask(\n    function=my_func,\n    task_backend_specific_kwargs={\n        \"process_template\": {\"policy\": policy}\n    },\n)\n\n# Parallel job (multiple process groups)\nComputeTask(\n    function=my_func,\n    task_backend_specific_kwargs={\n        \"process_templates\": [\n            (nranks_group_1, {\"policy\": policy_1}),\n            (nranks_group_2, {\"policy\": policy_2}),\n        ]\n    },\n)\n```\n\n### Return Values vs Stdout\n\n| Execution Mode | `task.return_value` | `task.stdout` |\n|---|---|---|\n| Function Native (no template) | Python return value | `None` |\n| Function Process (`process_template`) | Exit code (int) | Captured from `print()` |\n| Function Job (`process_templates`) | Exit code (int) | Captured from `print()` |\n| Executable Process/Job | `None` | Command stdout |\n\n**Tip:** When using `process_template` or `process_templates`, have your function `print(json.dumps(result))` and parse it with `json.loads(task.stdout.strip())`.\n\n### Execution Mode Summary\n\n| Task Config | Execution Mode | Dragon API |\n|-------------|---------------|------------|\n| `function` only | Function Native | `batch.function()` |\n| `function` + `process_template` | Function Process | `batch.process(ProcessTemplate(...))` |\n| `function` + `process_templates` | Function Job | `batch.job([(N, ProcessTemplate(...)), ...])` |\n| `executable` + `process_template` | Executable Process | `batch.process(ProcessTemplate(...))` |\n| `executable` + `process_templates` | Executable Job | `batch.job([(N, ProcessTemplate(...)), ...])` |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
