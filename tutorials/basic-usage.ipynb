{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# RHAPSODY Basic Usage Tutorial\n",
    "\n",
    "This tutorial walks you through the fundamentals of RHAPSODY using the **`ConcurrentExecutionBackend`** with a `ProcessPoolExecutor`. No HPC cluster or Dragon runtime needed — everything runs on your local machine.\n",
    "\n",
    "You will learn:\n",
    "\n",
    "1. How to run 100 function tasks and collect return values\n",
    "2. How to run 100 executable tasks and process stdout/stderr\n",
    "3. How to detect failed tasks and resubmit them\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install \"rhapsody-py\"\n",
    "pip install cloudpickle   # Required for ProcessPoolExecutor\n",
    "```\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "| Concept | What It Does |\n",
    "|---|---|\n",
    "| **`ComputeTask`** | A unit of work — a Python function or a shell command. |\n",
    "| **`ConcurrentExecutionBackend`** | Runs tasks locally using Python's `ProcessPoolExecutor` or `ThreadPoolExecutor`. |\n",
    "| **`Session`** | Connects tasks to backends, submits them, and tracks their lifecycle. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from rhapsody.api import ComputeTask, Session\n",
    "from rhapsody.backends import ConcurrentExecutionBackend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Running 100 Function Tasks\n",
    "\n",
    "Let's start by running 100 Python functions in parallel. Each function takes an integer, does some computation, and returns a result.\n",
    "\n",
    "After completion, every task object is updated **in-place** with:\n",
    "\n",
    "| Field | Description |\n",
    "|---|---|\n",
    "| `task.state` | `\"DONE\"` or `\"FAILED\"` |\n",
    "| `task.return_value` | The function's return value |\n",
    "| `task.exit_code` | `0` on success, `1` on failure |\n",
    "| `task.exception` | The exception object if the task failed |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square(n):\n",
    "    \"\"\"Compute the square of a number.\"\"\"\n",
    "    return {\"input\": n, \"result\": n * n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_100_functions():\n",
    "    # Create a backend with a ProcessPoolExecutor (4 workers)\n",
    "    backend = ConcurrentExecutionBackend(\n",
    "        executor=ProcessPoolExecutor(max_workers=4)\n",
    "    )\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    # Create 100 function tasks\n",
    "    tasks = [\n",
    "        ComputeTask(function=compute_square, args=(i,))\n",
    "        for i in range(100)\n",
    "    ]\n",
    "\n",
    "    async with session:\n",
    "        await session.submit_tasks(tasks)\n",
    "        await session.wait_tasks(tasks)\n",
    "\n",
    "        # Count results\n",
    "        done = [t for t in tasks if t.state == \"DONE\"]\n",
    "        failed = [t for t in tasks if t.state == \"FAILED\"]\n",
    "        print(f\"Done: {len(done)}, Failed: {len(failed)}\")\n",
    "\n",
    "        # Show first 5 results\n",
    "        for t in tasks[:5]:\n",
    "            print(f\"  {t.uid} -> {t.return_value}\")\n",
    "\n",
    "        # Verify all results\n",
    "        for t in done:\n",
    "            n = t.return_value[\"input\"]\n",
    "            assert t.return_value[\"result\"] == n * n\n",
    "        print(\"All 100 results verified.\")\n",
    "\n",
    "await run_100_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```\n",
    "Done: 100, Failed: 0\n",
    "  task.000001 -> {'input': 0, 'result': 0}\n",
    "  task.000002 -> {'input': 1, 'result': 1}\n",
    "  task.000003 -> {'input': 2, 'result': 4}\n",
    "  task.000004 -> {'input': 3, 'result': 9}\n",
    "  task.000005 -> {'input': 4, 'result': 16}\n",
    "All 100 results verified.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Running 100 Executable Tasks\n",
    "\n",
    "Now let's run 100 shell commands. For executable tasks, results are captured as:\n",
    "\n",
    "| Field | Description |\n",
    "|---|---|\n",
    "| `task.stdout` | Standard output (string) |\n",
    "| `task.stderr` | Standard error (string) |\n",
    "| `task.exit_code` | Process exit code (`0` = success) |\n",
    "| `task.state` | `\"DONE\"` if exit code is 0, `\"FAILED\"` otherwise |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_100_executables():\n",
    "    backend = ConcurrentExecutionBackend(\n",
    "        executor=ProcessPoolExecutor(max_workers=4)\n",
    "    )\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    # Create 100 executable tasks: each echoes its index\n",
    "    tasks = [\n",
    "        ComputeTask(\n",
    "            executable=\"/bin/echo\",\n",
    "            arguments=[f\"Task {i}: result={i * i}\"],\n",
    "        )\n",
    "        for i in range(100)\n",
    "    ]\n",
    "\n",
    "    async with session:\n",
    "        await session.submit_tasks(tasks)\n",
    "        await session.wait_tasks(tasks)\n",
    "\n",
    "        done = [t for t in tasks if t.state == \"DONE\"]\n",
    "        failed = [t for t in tasks if t.state == \"FAILED\"]\n",
    "        print(f\"Done: {len(done)}, Failed: {len(failed)}\")\n",
    "\n",
    "        # Show first 5 results\n",
    "        for t in tasks[:5]:\n",
    "            print(f\"  {t.uid} | exit_code={t.exit_code} | stdout={t.stdout.strip()!r}\")\n",
    "\n",
    "await run_100_executables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```\n",
    "Done: 100, Failed: 0\n",
    "  task.000001 | exit_code=0 | stdout='Task 0: result=0'\n",
    "  task.000002 | exit_code=0 | stdout='Task 1: result=1'\n",
    "  task.000003 | exit_code=0 | stdout='Task 2: result=4'\n",
    "  task.000004 | exit_code=0 | stdout='Task 3: result=9'\n",
    "  task.000005 | exit_code=0 | stdout='Task 4: result=16'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Processing stdout and stderr\n",
    "\n",
    "Let's run a command that writes to both stdout and stderr, and parse the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_stdout_stderr():\n",
    "    backend = ConcurrentExecutionBackend(\n",
    "        executor=ProcessPoolExecutor(max_workers=4)\n",
    "    )\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    # A command that writes to both stdout and stderr\n",
    "    tasks = [\n",
    "        ComputeTask(\n",
    "            executable=\"/bin/bash\",\n",
    "            arguments=[\n",
    "                \"-c\",\n",
    "                f\"echo 'stdout line {i}' && echo 'stderr warning {i}' >&2\",\n",
    "            ],\n",
    "        )\n",
    "        for i in range(5)\n",
    "    ]\n",
    "\n",
    "    async with session:\n",
    "        await session.submit_tasks(tasks)\n",
    "        await session.wait_tasks(tasks)\n",
    "\n",
    "        for t in tasks:\n",
    "            print(f\"{t.uid}:\")\n",
    "            print(f\"  stdout : {t.stdout.strip()!r}\")\n",
    "            print(f\"  stderr : {t.stderr.strip()!r}\")\n",
    "            print(f\"  exit   : {t.exit_code}\")\n",
    "            print()\n",
    "\n",
    "await process_stdout_stderr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Parsing structured output\n",
    "\n",
    "A common pattern is to have your command output JSON, then parse it from `task.stdout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "async def parse_json_output():\n",
    "    backend = ConcurrentExecutionBackend(\n",
    "        executor=ProcessPoolExecutor(max_workers=4)\n",
    "    )\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    # Commands that output JSON\n",
    "    tasks = [\n",
    "        ComputeTask(\n",
    "            executable=\"/bin/bash\",\n",
    "            arguments=[\n",
    "                \"-c\",\n",
    "                f'echo \\'{{\"id\": {i}, \"value\": {i * 10}}}\\'',\n",
    "            ],\n",
    "        )\n",
    "        for i in range(5)\n",
    "    ]\n",
    "\n",
    "    async with session:\n",
    "        await session.submit_tasks(tasks)\n",
    "        await session.wait_tasks(tasks)\n",
    "\n",
    "        # Parse JSON from stdout\n",
    "        for t in tasks:\n",
    "            data = json.loads(t.stdout.strip())\n",
    "            print(f\"  {t.uid} -> id={data['id']}, value={data['value']}\")\n",
    "\n",
    "await parse_json_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Error Handling and Task Resubmission\n",
    "\n",
    "Tasks can fail for many reasons — a function raises an exception, a command returns a non-zero exit code, or a timeout expires. RHAPSODY captures all of this on the task object so you can inspect the failure and decide what to do.\n",
    "\n",
    "### Detecting Failed Tasks\n",
    "\n",
    "After `wait_tasks()`, check each task's `state`:\n",
    "\n",
    "- `\"DONE\"` — task completed successfully\n",
    "- `\"FAILED\"` — task failed (check `task.exception` or `task.stderr`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sometimes_fails(n):\n",
    "    \"\"\"A function that fails for even numbers.\"\"\"\n",
    "    if n % 2 == 0:\n",
    "        raise ValueError(f\"Task {n} failed: even numbers not allowed!\")\n",
    "    return {\"input\": n, \"result\": n * n}\n",
    "\n",
    "\n",
    "async def detect_failures():\n",
    "    backend = ConcurrentExecutionBackend(\n",
    "        executor=ProcessPoolExecutor(max_workers=4)\n",
    "    )\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    tasks = [\n",
    "        ComputeTask(function=sometimes_fails, args=(i,))\n",
    "        for i in range(10)\n",
    "    ]\n",
    "\n",
    "    async with session:\n",
    "        await session.submit_tasks(tasks)\n",
    "        await session.wait_tasks(tasks)\n",
    "\n",
    "        for t in tasks:\n",
    "            if t.state == \"DONE\":\n",
    "                print(f\"  {t.uid} OK    -> {t.return_value}\")\n",
    "            else:\n",
    "                print(f\"  {t.uid} FAIL  -> {t.stderr}\")\n",
    "\n",
    "        done = [t for t in tasks if t.state == \"DONE\"]\n",
    "        failed = [t for t in tasks if t.state == \"FAILED\"]\n",
    "        print(f\"\\nSummary: {len(done)} done, {len(failed)} failed\")\n",
    "\n",
    "await detect_failures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Resubmitting Failed Tasks\n",
    "\n",
    "A practical pattern: run a batch, collect failures, create new tasks with a fix, and resubmit. Since task UIDs are auto-generated and unique, you simply create fresh `ComputeTask` objects for the retry.\n",
    "\n",
    "In the example below, tasks fail randomly. We catch the failures, create new tasks for the same inputs, and resubmit until everything succeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def flaky_function(n):\n",
    "    \"\"\"A function that fails randomly ~30% of the time.\"\"\"\n",
    "    if random.random() < 0.3:\n",
    "        raise RuntimeError(f\"Random failure for input {n}\")\n",
    "    return {\"input\": n, \"result\": n * 2}\n",
    "\n",
    "\n",
    "async def retry_failed_tasks():\n",
    "    backend = ConcurrentExecutionBackend(\n",
    "        executor=ProcessPoolExecutor(max_workers=4)\n",
    "    )\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    # Initial batch: 20 tasks\n",
    "    pending_inputs = list(range(20))\n",
    "    all_results = {}  # input -> return_value\n",
    "    max_retries = 5\n",
    "    attempt = 0\n",
    "\n",
    "    async with session:\n",
    "        while pending_inputs and attempt < max_retries:\n",
    "            attempt += 1\n",
    "            print(f\"\\n--- Attempt {attempt}: submitting {len(pending_inputs)} tasks ---\")\n",
    "\n",
    "            # Create tasks for remaining inputs\n",
    "            tasks = [\n",
    "                ComputeTask(function=flaky_function, args=(n,))\n",
    "                for n in pending_inputs\n",
    "            ]\n",
    "\n",
    "            await session.submit_tasks(tasks)\n",
    "            await session.wait_tasks(tasks)\n",
    "\n",
    "            # Separate successes and failures\n",
    "            still_pending = []\n",
    "            for task, input_val in zip(tasks, pending_inputs):\n",
    "                if task.state == \"DONE\":\n",
    "                    all_results[input_val] = task.return_value\n",
    "                else:\n",
    "                    print(f\"  {task.uid} failed: {task.stderr}\")\n",
    "                    still_pending.append(input_val)\n",
    "\n",
    "            pending_inputs = still_pending\n",
    "            print(f\"  Done: {len(all_results)}, Remaining: {len(pending_inputs)}\")\n",
    "\n",
    "    if pending_inputs:\n",
    "        print(f\"\\nGave up on {len(pending_inputs)} tasks after {max_retries} attempts.\")\n",
    "    else:\n",
    "        print(f\"\\nAll 20 tasks completed in {attempt} attempt(s).\")\n",
    "\n",
    "    # Verify results\n",
    "    for n, result in sorted(all_results.items()):\n",
    "        assert result[\"result\"] == n * 2\n",
    "    print(f\"Verified {len(all_results)} results.\")\n",
    "\n",
    "await retry_failed_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "**Expected output (varies due to randomness):**\n",
    "```\n",
    "--- Attempt 1: submitting 20 tasks ---\n",
    "  task.000003 failed: Random failure for input 2\n",
    "  task.000008 failed: Random failure for input 7\n",
    "  task.000015 failed: Random failure for input 14\n",
    "  Done: 17, Remaining: 3\n",
    "\n",
    "--- Attempt 2: submitting 3 tasks ---\n",
    "  task.000022 failed: Random failure for input 7\n",
    "  Done: 19, Remaining: 1\n",
    "\n",
    "--- Attempt 3: submitting 1 tasks ---\n",
    "  Done: 20, Remaining: 0\n",
    "\n",
    "All 20 tasks completed in 3 attempt(s).\n",
    "Verified 20 results.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Resubmitting Failed Executable Tasks\n",
    "\n",
    "The same pattern works for executable tasks. Here we use commands that fail based on exit codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retry_failed_executables():\n",
    "    backend = ConcurrentExecutionBackend(\n",
    "        executor=ProcessPoolExecutor(max_workers=4)\n",
    "    )\n",
    "    session = Session(backends=[backend])\n",
    "\n",
    "    # Mix of commands: some succeed (/bin/true), some fail (/bin/false)\n",
    "    inputs = list(range(10))\n",
    "    results = {}\n",
    "\n",
    "    async with session:\n",
    "        # First attempt: even-numbered tasks will use /bin/false (fail)\n",
    "        tasks = [\n",
    "            ComputeTask(\n",
    "                executable=\"/bin/bash\",\n",
    "                arguments=[\n",
    "                    \"-c\",\n",
    "                    f\"if [ $(( {i} % 2 )) -eq 0 ]; then exit 1; else echo 'Success {i}'; fi\",\n",
    "                ],\n",
    "            )\n",
    "            for i in inputs\n",
    "        ]\n",
    "\n",
    "        await session.submit_tasks(tasks)\n",
    "        await session.wait_tasks(tasks)\n",
    "\n",
    "        failed_inputs = []\n",
    "        for task, i in zip(tasks, inputs):\n",
    "            if task.state == \"DONE\":\n",
    "                results[i] = task.stdout.strip()\n",
    "            else:\n",
    "                print(f\"  Task for input {i} failed (exit_code={task.exit_code})\")\n",
    "                failed_inputs.append(i)\n",
    "\n",
    "        print(f\"\\nFirst attempt: {len(results)} done, {len(failed_inputs)} failed\")\n",
    "\n",
    "        # Retry failed tasks with a fixed command (always succeeds)\n",
    "        if failed_inputs:\n",
    "            print(f\"Retrying {len(failed_inputs)} failed tasks...\")\n",
    "            retry_tasks = [\n",
    "                ComputeTask(\n",
    "                    executable=\"/bin/echo\",\n",
    "                    arguments=[f\"Retry success {i}\"],\n",
    "                )\n",
    "                for i in failed_inputs\n",
    "            ]\n",
    "\n",
    "            await session.submit_tasks(retry_tasks)\n",
    "            await session.wait_tasks(retry_tasks)\n",
    "\n",
    "            for task, i in zip(retry_tasks, failed_inputs):\n",
    "                results[i] = task.stdout.strip()\n",
    "\n",
    "        print(f\"\\nFinal results ({len(results)} tasks):\")\n",
    "        for i in sorted(results):\n",
    "            print(f\"  Input {i}: {results[i]!r}\")\n",
    "\n",
    "await retry_failed_executables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "### Creating a Backend\n",
    "\n",
    "```python\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from rhapsody.backends import ConcurrentExecutionBackend\n",
    "\n",
    "# ProcessPoolExecutor — runs tasks in separate processes (true parallelism)\n",
    "backend = ConcurrentExecutionBackend(executor=ProcessPoolExecutor(max_workers=4))\n",
    "\n",
    "# ThreadPoolExecutor — runs tasks in threads (default if no executor given)\n",
    "backend = ConcurrentExecutionBackend()  # uses ThreadPoolExecutor\n",
    "```\n",
    "\n",
    "### Task Result Fields\n",
    "\n",
    "| Field | Function Tasks | Executable Tasks |\n",
    "|---|---|---|\n",
    "| `task.state` | `\"DONE\"` / `\"FAILED\"` | `\"DONE\"` / `\"FAILED\"` |\n",
    "| `task.return_value` | Function's return value | `None` |\n",
    "| `task.stdout` | String of return value | Command stdout |\n",
    "| `task.stderr` | Exception message on failure | Command stderr |\n",
    "| `task.exit_code` | `0` success, `1` failure | Process exit code |\n",
    "| `task.exception` | Exception object on failure | `None` |\n",
    "\n",
    "### Error Handling Pattern\n",
    "\n",
    "```python\n",
    "await session.submit_tasks(tasks)\n",
    "await session.wait_tasks(tasks)\n",
    "\n",
    "done = [t for t in tasks if t.state == \"DONE\"]\n",
    "failed = [t for t in tasks if t.state == \"FAILED\"]\n",
    "\n",
    "# Inspect failures\n",
    "for t in failed:\n",
    "    print(f\"{t.uid}: {t.stderr}\")  # Error message\n",
    "    print(f\"  Exception: {t.exception}\")  # Exception object (function tasks)\n",
    "\n",
    "# Resubmit with new ComputeTask objects\n",
    "retry_tasks = [ComputeTask(function=my_func, args=(original_input,)) for ...]\n",
    "await session.submit_tasks(retry_tasks)\n",
    "await session.wait_tasks(retry_tasks)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
